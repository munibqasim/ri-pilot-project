================================================================================
RI PILOT PROJECT - RESILIENT INFRASTRUCTURE EXTRACTION PIPELINE
================================================================================

OVERVIEW:
Multi-phase AI pipeline for extracting resilient infrastructure interventions 
from World Bank Project Appraisal Documents (PADs). Uses keyword search, 
semantic embeddings, deduplication, and Claude via AWS Bedrock.

================================================================================
QUICK START:
================================================================================

1. Install Dependencies:
   pip install spacy nltk sentence-transformers torch boto3 --break-system-packages
   python -m spacy download en_core_web_sm
   python -c "import nltk; nltk.download('punkt'); nltk.download('wordnet')"

2. Configure AWS Settings:
   Edit config.py and update:
   - s3_bucket: "your-bucket-name"
   - s3_input_prefix and s3_output_prefix
   - AWS credentials configured via 'aws configure'

3. Run the Pipeline:
   python run_keyword_search.py your_pad1.txt your_pad2.txt
   python run_semantic_search.py your_pad1.txt your_pad2.txt
   python run_deduplication.py
   python run_bedrock_classification.py
   python run_results_processing.py
   python run_activity_classification.py

4. Get Results:
   Open: final_ri_classifications.json

================================================================================
WHAT THIS PIPELINE DOES:
================================================================================

Phase 1: KEYWORD SEARCH
  - Searches PAD documents for RI-related keywords
  - Uses lemmatization to catch variations (flood, flooding, floods)
  - Extracts context around matches (200-1000 characters)
  - Output: keyword_search_results.json

Phase 2: SEMANTIC SEARCH  
  - Uses AI embeddings to find semantically similar content
  - Searches with 8 sector-specific queries (energy, transport, water, etc.)
  - Finds content that means "resilient infrastructure" without exact keywords
  - Output: semantic_search_results.json

Phase 3: DEDUPLICATION
  - Combines results from both search methods
  - Removes overlapping chunks (>50% overlap)
  - Tracks which method found each chunk
  - Output: combined_deduplicated_results.json

Phase 4: BEDROCK CLASSIFICATION
  - Sends chunks to Claude (AWS Bedrock) for classification
  - Classifies as POSITIVE (real RI) or NEGATIVE (false positive)
  - Provides confidence level and reasoning
  - Output: bedrock_classifications.json

Phase 5: RESULTS PROCESSING
  - Enriches classifications with original chunk metadata
  - Filters for positive RI interventions only
  - Output: final_enriched_results.json & final_enriched_results_positive_only.json

Phase 6: ACTIVITY CLASSIFICATION
  - Classifies positive RI excerpts into 8 activity categories
  - Engineering Design, Asset Management, Contingency Planning, etc.
  - Optional deduplication based on text similarity (85-90% threshold)
  - Output: final_ri_classifications.json

================================================================================
CUSTOMIZATION:
================================================================================

Add/Modify Keywords:
  Edit config.py → RI_KEYWORDS dictionary
  Organized by sector: energy, transport, water, ICT, etc.

Add/Modify Semantic Queries:
  Edit config.py → SEMANTIC_QUERIES list
  8 pre-defined queries covering different RI aspects

Adjust Parameters:
  Edit config.py → SEARCH_PARAMS
  - keyword_context_sentences: 3 (sentences before/after match)
  - semantic_chunk_size: 500 (characters per chunk)
  - dedup_overlap_threshold: 0.5 (50% overlap to merge)
  - batch_size: 5 (chunks per Bedrock request)

Change AWS Settings:
  Edit config.py → AWS_CONFIG
  - region, s3_bucket, model_id, etc.

================================================================================
OUTPUT FILES:
================================================================================

keyword_search_results.json          - Raw keyword matches from PADs
semantic_search_results.json         - Raw semantic matches from PADs  
combined_deduplicated_results.json   - Merged and deduplicated chunks
bedrock_classifications.json         - Classification results from Claude
final_enriched_results.json          - All chunks with classifications
final_enriched_results_positive_only.json - Only positive RI interventions
final_ri_classifications.json        - Positive interventions with activity types ⭐

The last file is what you want - it contains the confirmed resilient
infrastructure interventions categorized by activity type.

================================================================================
TYPICAL WORKFLOW:
================================================================================

For a New Project:
1. Place your PAD .txt files in a working directory
2. Run Phase 1 & 2 on all PAD files
3. Run Phase 3 to deduplicate
4. Run Phase 4 to classify (requires AWS Bedrock access)
5. Run Phase 5 to get final results
6. Review final_enriched_results_positive_only.json

For Iterating/Testing:
- Test new keywords: Edit config.py, re-run Phase 1 only
- Test new queries: Edit config.py, re-run Phase 2 only
- Adjust deduplication: Edit config.py, re-run Phase 3 only
- Review classifications: Just re-run Phase 5 with existing results

For Adding More PADs:
- Run Phase 1 & 2 on new PADs
- Results append to existing JSON files
- Re-run Phase 3-5 to include new PADs

================================================================================
TROUBLESHOOTING:
================================================================================

"File not found: keyword_search_results.json"
→ Run Phase 1 first: python run_keyword_search.py your_files.txt

"Authentication failed for AWS"
→ Configure AWS credentials: aws configure
→ Ensure Bedrock access is enabled in your AWS account

"ModuleNotFoundError: No module named 'spacy'"
→ Install dependencies: pip install spacy --break-system-packages

"No matches found"
→ Check that your input files are plain text (.txt)
→ Try expanding keywords in config.py

"Bedrock batch job failed"
→ Check S3 bucket permissions
→ Verify IAM role: BedrockBatchInferenceRole exists
→ Confirm Claude Sonnet 4 model access

================================================================================
REQUIREMENTS:
================================================================================

Python Dependencies:
- spacy (NLP processing)
- nltk (text processing)
- sentence-transformers (semantic search)
- torch (ML backend)
- boto3 (AWS SDK)

AWS Requirements:
- AWS account with Bedrock access
- S3 bucket for batch processing
- IAM role: BedrockBatchInferenceRole
- Claude Sonnet 4 model access

Input Format:
- Plain text files (.txt)
- UTF-8 encoding
- PAD documents from World Bank projects

================================================================================
DOCUMENTATION:
================================================================================

README.md       - Complete technical documentation
QUICKSTART.md   - Condensed getting started guide  
This file       - Overview and usage guide

================================================================================
SUPPORT:
================================================================================

For questions about:
- Methodology: See README.md technical details
- Quick setup: See QUICKSTART.md
- AWS/Bedrock: Check AWS documentation
- Code issues: Review module docstrings in .py files

Repository: https://github.com/munibqasim/ri-pilot-project

================================================================================
